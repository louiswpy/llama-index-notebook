{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c1472cb-ab1f-48fc-b8d2-e369baee4557",
   "metadata": {},
   "source": [
    "## Setting LLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d45b36d-757f-4acd-8083-995138095f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0b5c8e0-4619-40e1-9141-f3196290ff92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "#print(\"Open AI - \",os.getenv(\"LITELLM_URL\"),os.getenv(\"OPENAI_API_MODEL\"), os.getenv(\"OPENAI_API_EMBEDDING\"))\n",
    "#print(\"OLLAMA  - \",os.getenv(\"OLLAMA_URL\"),os.getenv(\"OLLAMA_MODEL\"))\n",
    "#print(\"Local OLLAMA - \",os.getenv(\"OLLAMA_LOCAL_URL\"),os.getenv(\"OLLAMA_LOCAL_MODEL\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6c5c71d-7466-40e9-a6d9-e62837f621ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b8a433aa-4f89-4e06-8acd-780e730536bd",
   "metadata": {},
   "source": [
    "### RUN LLM AS OLLAMA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4008df15-e79d-4fb2-8269-08182ae74b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#install OPEN AI LLM, skip if already installed\n",
    "!pipenv install llama-index-llms-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdfa4da-e8ed-4b0f-8742-d53365333172",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "api_base=os.getenv(\"OLLAMA_LOCAL_URL\")\n",
    "model=os.getenv(\"OLLAMA_LOCAL_MODEL\")\n",
    "llm = Ollama(model=model, base_url=api_base,request_timeout=120.0)\n",
    "\n",
    "# use remote ollam\n",
    "\"\"\"\n",
    "api_base=os.getenv(\"OLLAMA_URL\")\n",
    "model=os.getenv(\"OLLAMA_MODEL\")\n",
    "llm = Ollama(model=model, base_url=api_base,request_timeout=180.0)\n",
    "\"\"\"\n",
    "#test run\n",
    "response = llm.complete(\"What is the capital of France?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34bf597-435b-464d-afc4-49f852d60479",
   "metadata": {},
   "source": [
    "### RUN LLM AS OPEN AI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361f4e3c-18b1-4710-9a7d-dbb87fbf0f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#install OPEN AI LLM, skip if already installed\n",
    "!pipenv install llama-index-llms-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cca4af2b-3cbf-42b5-8a0c-eccb3e796188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "api_base=os.getenv(\"LITELLM_URL\")\n",
    "model=os.getenv(\"OPENAI_API_MODEL\")\n",
    "\n",
    "Settings.llm = OpenAI(\n",
    "    model=model,\n",
    "    api_base = api_base,\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "resp = Settings.llm.complete(\"What is the capital of France?\")\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f57c66-5521-4e24-9197-07ce916f7491",
   "metadata": {},
   "source": [
    "## Setting Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f1d1b49-087e-49e3-906b-da53d09fd1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use open AI embedding\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "api_base=os.getenv(\"LITELLM_URL\")\n",
    "embedding_model=os.getenv(\"OPENAI_API_EMBEDDING\")\n",
    "\n",
    "Settings.embed_model = OpenAIEmbedding(\n",
    "    model_name=embedding_model,\n",
    "    api_base = api_base,\n",
    ")\n",
    "\n",
    "# embed_text = Settings.embed_model.get_text_embedding(\"hello\")\n",
    "# print(f\"{len(embed_text)}, {embed_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed08805f-6e69-4952-b701-59cdf35ddf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "documents = SimpleDirectoryReader(\"./data/paul\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cd28b56-ba06-4c70-a66c-67908a4ece59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "#logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "#logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68817bc1-db14-4292-afb7-44929fa5ddc3",
   "metadata": {},
   "source": [
    "### Chat Engine\n",
    "it is a stateful analogy of a Query Engine. By keeping track of the conversation history, it can answer questions with past context in mind.\n",
    "\n",
    "- best - Turn the query engine into a tool, for use with a ReAct data agent or an OpenAI data agent, depending on what your LLM supports. OpenAI data agents require gpt-3.5-turbo or gpt-4 as they use the function calling API from OpenAI.\n",
    "- condense_question - Look at the chat history and re-write the user message to be a query for the index. Return the response after reading the response from the query engine.\n",
    "- context - Retrieve nodes from the index using every user message. The retrieved text is inserted into the system prompt, so that the chat engine can either respond naturally or use the context from the query engine.\n",
    "- condense_plus_context - A combination of condense_question and context. Look at the chat history and re-write the user message to be a retrieval query for the index. The retrieved text is inserted into the system prompt, so that the chat engine can either respond naturally or use the context from the query engine.\n",
    "- simple - A simple chat with the LLM directly, no query engine involved.\n",
    "- react - Same as best, but forces a ReAct data agent.\n",
    "- openai - Same as best, but forces an OpenAI data agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cef36f9d-3ece-42a7-a913-a0f3c701896f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "documents = SimpleDirectoryReader(\"./data/paul\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4cd3cbd-ae2c-4d6d-8634-cc2a7d8f155c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "parser = SentenceSplitter(chunk_size=1024, chunk_overlap=20)\n",
    "nodes = parser.get_nodes_from_documents(documents)\n",
    "index = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35ffbc13-61d1-4bb6-98d3-ab181ded5e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_engine = index.as_chat_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42d9add4-ee5a-426d-95f9-a7458b9315ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why don't scientists trust atoms?\n",
      "\n",
      "Because they make up everything!\n"
     ]
    }
   ],
   "source": [
    "response = chat_engine.chat(\"Tell me a joke.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3223e246-7d62-44a8-8d8d-835aed76ae7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Entering Chat REPL =====\n",
      "Type \"exit\" to exit.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Human:  hi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Hello! How can I assist you today?\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Human:  exit\n"
     ]
    }
   ],
   "source": [
    "chat_engine.chat_repl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "084eba68-e341-4bc5-a9dc-dbcd6aab0a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Entering Chat REPL =====\n",
      "Type \"exit\" to exit.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Human:  Hi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying with: Hi\n",
      "Assistant: Hello! How can I assist you today?\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Human:  Exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying with: Could you please help me with how to exit or close this conversation?\n",
      "Assistant: To exit or close the conversation, you might say something like, \"Thank you for the discussion; I appreciate your insights. I need to wrap things up now.\" Alternatively, you could express gratitude and indicate that you have other commitments, such as, \"I really enjoyed our conversation, but I have to attend to some other matters now. Let's catch up later.\"\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Human:  exit\n"
     ]
    }
   ],
   "source": [
    "chat_engine = index.as_chat_engine(chat_mode=\"condense_question\", verbose=True)\n",
    "chat_engine.chat_repl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "058444ef-a8a9-4592-9c24-2da0fee9d935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Entering Chat REPL =====\n",
      "Type \"exit\" to exit.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Human:  hi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Hello! How can I assist you today?\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Human:  exit\n"
     ]
    }
   ],
   "source": [
    "chat_engine = index.as_chat_engine(chat_mode=\"simple\", verbose=True)\n",
    "chat_engine.chat_repl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a308523-b227-49d9-ab2d-17b93cf46a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Entering Chat REPL =====\n",
      "Type \"exit\" to exit.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Human:  hoi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Hoi! How can I assist you today?\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Human:  exit\n"
     ]
    }
   ],
   "source": [
    "chat_engine = index.as_chat_engine(chat_mode=\"context\", verbose=True)\n",
    "chat_engine.chat_repl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66b593f4-1588-40c5-98e8-a4574e129515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Entering Chat REPL =====\n",
      "Type \"exit\" to exit.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Human:  hi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: hi\n",
      "Assistant: Hello! How can I assist you today?\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Human:  exit\n"
     ]
    }
   ],
   "source": [
    "chat_engine = index.as_chat_engine(chat_mode=\"react\", verbose=True)\n",
    "chat_engine.chat_repl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "488b920f-da3f-4fd5-9a84-9abf6471a056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Entering Chat REPL =====\n",
      "Type \"exit\" to exit.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Human:  hio\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying with: hio\n",
      "Assistant: Hello! How can I assist you today?\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Human:  exit\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.core.llms import ChatMessage, MessageRole\n",
    "from llama_index.core.chat_engine import CondenseQuestionChatEngine\n",
    "\n",
    "custom_prompt = PromptTemplate(\n",
    "    \"\"\"\\\n",
    "Given a conversation (between Human and Assistant) and a follow up message from Human, \\\n",
    "rewrite the message to be a standalone question that captures all relevant context \\\n",
    "from the conversation.\n",
    "\n",
    "<Chat History>\n",
    "{chat_history}\n",
    "\n",
    "<Follow Up Message>\n",
    "{question}\n",
    "\n",
    "<Standalone question>\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# list of `ChatMessage` objects\n",
    "custom_chat_history = [\n",
    "    ChatMessage(\n",
    "        role=MessageRole.USER,\n",
    "        content=\"Hello assistant, we are having a insightful discussion about Paul Graham today.\",\n",
    "    ),\n",
    "    ChatMessage(role=MessageRole.ASSISTANT, content=\"Okay, sounds good.\"),\n",
    "]\n",
    "\n",
    "query_engine = index.as_query_engine()\n",
    "chat_engine = CondenseQuestionChatEngine.from_defaults(\n",
    "    query_engine=query_engine,\n",
    "    condense_question_prompt=custom_prompt,\n",
    "    chat_history=custom_chat_history,\n",
    "    verbose=True,\n",
    ")\n",
    "chat_engine.chat_repl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbc1431-7692-4601-9d5a-5f050feab148",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama-index-2",
   "language": "python",
   "name": "llama-index-2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
