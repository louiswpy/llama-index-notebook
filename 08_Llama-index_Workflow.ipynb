{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c1472cb-ab1f-48fc-b8d2-e369baee4557",
   "metadata": {},
   "source": [
    "## Setting LLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d45b36d-757f-4acd-8083-995138095f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0b5c8e0-4619-40e1-9141-f3196290ff92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "#print(\"Open AI - \",os.getenv(\"LITELLM_URL\"),os.getenv(\"OPENAI_API_MODEL\"), os.getenv(\"OPENAI_API_EMBEDDING\"))\n",
    "#print(\"OLLAMA  - \",os.getenv(\"OLLAMA_URL\"),os.getenv(\"OLLAMA_MODEL\"))\n",
    "#print(\"Local OLLAMA - \",os.getenv(\"OLLAMA_LOCAL_URL\"),os.getenv(\"OLLAMA_LOCAL_MODEL\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6c5c71d-7466-40e9-a6d9-e62837f621ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b8a433aa-4f89-4e06-8acd-780e730536bd",
   "metadata": {},
   "source": [
    "### RUN LLM AS OLLAMA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4008df15-e79d-4fb2-8269-08182ae74b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#install OPEN AI LLM, skip if already installed\n",
    "!pipenv install llama-index-llms-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "914bf946-ca90-4c4d-8748-01e1109e351e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is **Paris**. ðŸ—¼  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "api_base=os.getenv(\"OLLAMA_LOCAL_URL\")\n",
    "model=os.getenv(\"OLLAMA_LOCAL_MODEL\")\n",
    "llm = Ollama(model=model, base_url=api_base,request_timeout=120.0)\n",
    "\n",
    "# use remote ollam\n",
    "\"\"\"\n",
    "api_base=os.getenv(\"OLLAMA_URL\")\n",
    "model=os.getenv(\"OLLAMA_MODEL\")\n",
    "llm = Ollama(model=model, base_url=api_base,request_timeout=180.0)\n",
    "\"\"\"\n",
    "#test run\n",
    "response = llm.complete(\"What is the capital of France?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34bf597-435b-464d-afc4-49f852d60479",
   "metadata": {},
   "source": [
    "### RUN LLM AS OPEN AI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361f4e3c-18b1-4710-9a7d-dbb87fbf0f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#install OPEN AI LLM, skip if already installed\n",
    "!pipenv install llama-index-llms-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cca4af2b-3cbf-42b5-8a0c-eccb3e796188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "api_base=os.getenv(\"LITELLM_URL\")\n",
    "model=os.getenv(\"OPENAI_API_MODEL\")\n",
    "\n",
    "Settings.llm = OpenAI(\n",
    "    model=model,\n",
    "    api_base = api_base,\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "resp = Settings.llm.complete(\"What is the capital of France?\")\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f57c66-5521-4e24-9197-07ce916f7491",
   "metadata": {},
   "source": [
    "## Setting Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f1d1b49-087e-49e3-906b-da53d09fd1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use open AI embedding\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "api_base=os.getenv(\"LITELLM_URL\")\n",
    "embedding_model=os.getenv(\"OPENAI_API_EMBEDDING\")\n",
    "\n",
    "Settings.embed_model = OpenAIEmbedding(\n",
    "    model_name=embedding_model,\n",
    "    api_base = api_base,\n",
    ")\n",
    "\n",
    "# embed_text = Settings.embed_model.get_text_embedding(\"hello\")\n",
    "# print(f\"{len(embed_text)}, {embed_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed08805f-6e69-4952-b701-59cdf35ddf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "documents = SimpleDirectoryReader(\"./data/paul\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cd28b56-ba06-4c70-a66c-67908a4ece59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "#logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "#logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581b4678-6e8e-406a-bef6-687c87b63acc",
   "metadata": {},
   "source": [
    "### Workflows\n",
    "A Workflow in LlamaIndex is an event-driven abstraction used to chain together several events. Workflows are made up of steps, with each step responsible for handling certain event types and emitting new events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7acc1289-6c8e-4388-a3c3-4bf75e1ea9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import (\n",
    "    Event,\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    Workflow,\n",
    "    step,\n",
    "    Context,\n",
    ")\n",
    "import random\n",
    "from llama_index.core.workflow import draw_all_possible_flows\n",
    "from llama_index.utils.workflow import draw_most_recent_execution\n",
    "from llama_index.llms.openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3223e246-7d62-44a8-8d8d-835aed76ae7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaIndex, formerly known as GPT Index, is a framework designed to facilitate the integration of large language models (LLMs) with external data sources. It provides tools and structures to help developers create applications that can efficiently retrieve and process information from various data sources, such as databases, documents, and APIs, and then use that information in conjunction with LLMs.\n",
      "\n",
      "The main features of LlamaIndex include:\n",
      "\n",
      "1. **Data Integration**: It allows users to connect and pull data from different sources, making it easier to work with diverse datasets.\n",
      "\n",
      "2. **Indexing**: The framework helps in indexing the data, which improves the efficiency of information retrieval when interacting with LLMs.\n",
      "\n",
      "3. **Querying**: LlamaIndex provides mechanisms for querying the indexed data, enabling users to formulate questions and retrieve relevant information effectively.\n",
      "\n",
      "4. **LLM Interaction**: It streamlines the process of using LLMs to generate responses based on the retrieved data, enhancing the capabilities of applications built on top of LLMs.\n",
      "\n",
      "Overall, LlamaIndex aims to bridge the gap between LLMs and structured or unstructured data, making it easier for developers to build intelligent applications that leverage the power of language models while accessing real-world information.\n"
     ]
    }
   ],
   "source": [
    "class OpenAIGenerator(Workflow):\n",
    "    @step\n",
    "    async def generate(self, ev: StartEvent) -> StopEvent:\n",
    "        llm = Settings.llm\n",
    "        response = await llm.acomplete(ev.query)\n",
    "        return StopEvent(result=str(response))\n",
    "\n",
    "\n",
    "w = OpenAIGenerator(timeout=10, verbose=False)\n",
    "result = await w.run(query=\"What's LlamaIndex?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0dac9d27-5db3-4f28-9402-3504a26ecb14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\louis\\AppData\\Local\\Temp\\ipykernel_21864\\3792248423.py:1: DeprecationWarning: Call to deprecated function (or staticmethod) draw_all_possible_flows. (Install `llama-index-utils-workflow` and use the import `from llama_index.utils.workflow` instead.)\n",
      "  draw_all_possible_flows(OpenAIGenerator, filename=\"trivial_workflow.html\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trivial_workflow.html\n"
     ]
    }
   ],
   "source": [
    "draw_all_possible_flows(OpenAIGenerator, filename=\"trivial_workflow.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b105c46-0e21-44cd-8e82-6a45b430bf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FailedEvent(Event):\n",
    "    error: str\n",
    "\n",
    "\n",
    "class QueryEvent(Event):\n",
    "    query: str\n",
    "\n",
    "\n",
    "class LoopExampleFlow(Workflow):\n",
    "    @step\n",
    "    async def answer_query(\n",
    "        self, ev: StartEvent | QueryEvent\n",
    "    ) -> FailedEvent | StopEvent:\n",
    "        query = ev.query\n",
    "        print(query)\n",
    "        # try to answer the query\n",
    "        random_number = random.randint(0, 1)\n",
    "        if random_number == 0:\n",
    "            return FailedEvent(error=\"Failed to answer the query.\")\n",
    "        else:\n",
    "            return StopEvent(result=\"The answer to your query\")\n",
    "\n",
    "    @step\n",
    "    async def improve_query(self, ev: FailedEvent) -> QueryEvent | StopEvent:\n",
    "        # improve the query or decide it can't be fixed\n",
    "        random_number = random.randint(0, 1)\n",
    "        if random_number == 0:\n",
    "            return QueryEvent(query=\"Here's a better query.\")\n",
    "        else:\n",
    "            return StopEvent(result=\"Your query can't be fixed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76860aea-160a-4d8f-bdc9-1afa9f8f81f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\louis\\AppData\\Local\\Temp\\ipykernel_21864\\2055033644.py:1: DeprecationWarning: Call to deprecated function (or staticmethod) draw_all_possible_flows. (Install `llama-index-utils-workflow` and use the import `from llama_index.utils.workflow` instead.)\n",
      "  draw_all_possible_flows(LoopExampleFlow, filename=\"loop_workflow.html\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loop_workflow.html\n"
     ]
    }
   ],
   "source": [
    "\n",
    "draw_all_possible_flows(LoopExampleFlow, filename=\"loop_workflow.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37e4afa2-e3c4-485b-a156-143c0b2e9728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step answer_query\n",
      "What's LlamaIndex?\n",
      "Step answer_query produced event FailedEvent\n",
      "Running step improve_query\n",
      "Step improve_query produced event QueryEvent\n",
      "Running step answer_query\n",
      "Here's a better query.\n",
      "Step answer_query produced event FailedEvent\n",
      "Running step improve_query\n",
      "Step improve_query produced event QueryEvent\n",
      "Running step answer_query\n",
      "Here's a better query.\n",
      "Step answer_query produced event StopEvent\n",
      "The answer to your query\n"
     ]
    }
   ],
   "source": [
    "w = LoopExampleFlow(timeout=10, verbose=True)\n",
    "result = await w.run(query=\"What's LlamaIndex?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f315f059-a420-4ddb-9351-96887c26f6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalExampleFlow(Workflow):\n",
    "    @step\n",
    "    async def setup(self, ctx: Context, ev: StartEvent) -> QueryEvent:\n",
    "        # load our data here\n",
    "        await ctx.set(\"some_database\", [\"value1\", \"value2\", \"value3\"])\n",
    "\n",
    "        return QueryEvent(query=ev.query)\n",
    "\n",
    "    @step\n",
    "    async def query(self, ctx: Context, ev: QueryEvent) -> StopEvent:\n",
    "        # use our data with our query\n",
    "        data = await ctx.get(\"some_database\")\n",
    "\n",
    "        result = f\"The answer to your query is {data[1]}\"\n",
    "        return StopEvent(result=result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aff3cb4f-eba3-4477-ace5-626fc216701a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step setup\n",
      "Step setup produced event QueryEvent\n",
      "Running step query\n",
      "Step query produced event StopEvent\n",
      "The answer to your query is value2\n"
     ]
    }
   ],
   "source": [
    "g = GlobalExampleFlow(timeout=10, verbose=True)\n",
    "result = await g.run(query=\"What's LlamaIndex?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "61987ac0-1fbc-4f8a-8311-c08242f03cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaitExampleFlow(Workflow):\n",
    "    @step\n",
    "    async def setup(self, ctx: Context, ev: StartEvent) -> StopEvent:\n",
    "        if hasattr(ev, \"data\"):\n",
    "            await ctx.set(\"data\", ev.data)\n",
    "\n",
    "        return StopEvent(result=None)\n",
    "\n",
    "    @step\n",
    "    async def query(self, ctx: Context, ev: StartEvent) -> StopEvent:\n",
    "        if hasattr(ev, \"query\"):\n",
    "            # do we have any data?\n",
    "            if hasattr(self, \"data\"):\n",
    "                data = await ctx.get(\"data\")\n",
    "                return StopEvent(result=f\"Got the data {data}\")\n",
    "            else:\n",
    "                # there's non data yet\n",
    "                return None\n",
    "        else:\n",
    "            # this isn't a query\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d9ddb69-26fd-4988-8ada-eb7c8ff70ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step query\n",
      "Step query produced no event\n",
      "Running step setup\n",
      "Step setup produced event StopEvent\n",
      "No you can't\n",
      "---\n",
      "Running step query\n",
      "Step query produced no event\n",
      "Running step setup\n",
      "Step setup produced event StopEvent\n",
      "---\n",
      "Running step query\n",
      "Step query produced no event\n",
      "Running step setup\n",
      "Step setup produced event StopEvent\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "w = WaitExampleFlow(verbose=True)\n",
    "result = await w.run(query=\"Can I kick it?\")\n",
    "if result is None:\n",
    "    print(\"No you can't\")\n",
    "print(\"---\")\n",
    "result = await w.run(data=\"Yes you can\")\n",
    "print(\"---\")\n",
    "result = await w.run(query=\"Can I kick it?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b0207efa-dc88-45ba-93e6-54314b01a078",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEvent(Event):\n",
    "    input: str\n",
    "\n",
    "\n",
    "class SetupEvent(Event):\n",
    "    error: bool\n",
    "\n",
    "\n",
    "class QueryEvent(Event):\n",
    "    query: str\n",
    "\n",
    "\n",
    "class CollectExampleFlow(Workflow):\n",
    "    @step\n",
    "    async def setup(self, ctx: Context, ev: StartEvent) -> SetupEvent:\n",
    "        # generically start everything up\n",
    "        if not hasattr(self, \"setup\") or not self.setup:\n",
    "            self.setup = True\n",
    "            print(\"I got set up\")\n",
    "        return SetupEvent(error=False)\n",
    "\n",
    "    @step\n",
    "    async def collect_input(self, ev: StartEvent) -> InputEvent:\n",
    "        if hasattr(ev, \"input\"):\n",
    "            # perhaps validate the input\n",
    "            print(\"I got some input\")\n",
    "            return InputEvent(input=ev.input)\n",
    "\n",
    "    @step\n",
    "    async def parse_query(self, ev: StartEvent) -> QueryEvent:\n",
    "        if hasattr(ev, \"query\"):\n",
    "            # parse the query in some way\n",
    "            print(\"I got a query\")\n",
    "            return QueryEvent(query=ev.query)\n",
    "\n",
    "    @step\n",
    "    async def run_query(\n",
    "        self, ctx: Context, ev: InputEvent | SetupEvent | QueryEvent\n",
    "    ) -> StopEvent | None:\n",
    "        ready = ctx.collect_events(ev, [QueryEvent, InputEvent, SetupEvent])\n",
    "        if ready is None:\n",
    "            print(\"Not enough events yet\")\n",
    "            return None\n",
    "\n",
    "        # run the query\n",
    "        print(\"Now I have all the events\")\n",
    "        print(ready)\n",
    "\n",
    "        result = f\"Ran query '{ready[0].query}' on input '{ready[1].input}'\"\n",
    "        return StopEvent(result=result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ed2bc5a6-e616-4e5c-8332-e3d9c0f9c88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I got some input\n",
      "I got a query\n",
      "Not enough events yet\n",
      "Not enough events yet\n",
      "Now I have all the events\n",
      "[QueryEvent(query=\"Here's my question\"), InputEvent(input=\"Here's some input\"), SetupEvent(error=False)]\n",
      "Ran query 'Here's my question' on input 'Here's some input'\n"
     ]
    }
   ],
   "source": [
    "c = CollectExampleFlow()\n",
    "result = await c.run(input=\"Here's some input\", query=\"Here's my question\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6aa859-22c0-440c-bb44-d5b23a3f2f47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama-index-2",
   "language": "python",
   "name": "llama-index-2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
