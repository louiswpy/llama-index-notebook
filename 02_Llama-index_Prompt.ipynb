{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c1472cb-ab1f-48fc-b8d2-e369baee4557",
   "metadata": {},
   "source": [
    "## Setting LLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d45b36d-757f-4acd-8083-995138095f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0b5c8e0-4619-40e1-9141-f3196290ff92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "#print(\"Open AI - \",os.getenv(\"LITELLM_URL\"),os.getenv(\"OPENAI_API_MODEL\"), os.getenv(\"OPENAI_API_EMBEDDING\"))\n",
    "#print(\"OLLAMA  - \",os.getenv(\"OLLAMA_URL\"),os.getenv(\"OLLAMA_MODEL\"))\n",
    "#print(\"Local OLLAMA - \",os.getenv(\"OLLAMA_LOCAL_URL\"),os.getenv(\"OLLAMA_LOCAL_MODEL\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6c5c71d-7466-40e9-a6d9-e62837f621ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b8a433aa-4f89-4e06-8acd-780e730536bd",
   "metadata": {},
   "source": [
    "### RUN LLM AS OLLAMA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f23a2a4-876f-4e37-86fd-13e5f256f1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#install OPEN AI LLM, skip if already installed\n",
    "!pipenv install llama-index-llms-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "88433662-bed6-4bdf-8991-f02f730792b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is **Paris**. ðŸ‡«ðŸ‡·  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "api_base=os.getenv(\"OLLAMA_LOCAL_URL\")\n",
    "model=os.getenv(\"OLLAMA_LOCAL_MODEL\")\n",
    "llm = Ollama(model=model, base_url=api_base,request_timeout=120.0)\n",
    "\n",
    "# use remote ollam\n",
    "\"\"\"\n",
    "api_base=os.getenv(\"OLLAMA_URL\")\n",
    "model=os.getenv(\"OLLAMA_MODEL\")\n",
    "llm = Ollama(model=model, base_url=api_base,request_timeout=180.0)\n",
    "\"\"\"\n",
    "#test run\n",
    "response = llm.complete(\"What is the capital of France?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34bf597-435b-464d-afc4-49f852d60479",
   "metadata": {},
   "source": [
    "### RUN LLM AS OPEN AI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffc7c02-ad26-417b-8928-760a1e79898b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#install OPEN AI LLM, skip if already installed\n",
    "!pipenv install llama-index-llms-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cca4af2b-3cbf-42b5-8a0c-eccb3e796188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "api_base=os.getenv(\"LITELLM_URL\")\n",
    "model=os.getenv(\"OPENAI_API_MODEL\")\n",
    "\n",
    "Settings.llm = OpenAI(\n",
    "    model=model,\n",
    "    api_base = api_base,\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "resp = Settings.llm.complete(\"What is the capital of France?\")\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f57c66-5521-4e24-9197-07ce916f7491",
   "metadata": {},
   "source": [
    "## Setting Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f1d1b49-087e-49e3-906b-da53d09fd1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use open AI embedding\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "api_base=os.getenv(\"LITELLM_URL\")\n",
    "embedding_model=os.getenv(\"OPENAI_API_EMBEDDING\")\n",
    "\n",
    "Settings.embed_model = OpenAIEmbedding(\n",
    "    model_name=embedding_model,\n",
    "    api_base = api_base,\n",
    ")\n",
    "#embed_text = Settings.embed_model.get_text_embedding(\"hello\")\n",
    "#print(f\"{len(embed_text)}, {embed_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf0bff1-afdb-412e-b679-ab31f0341f89",
   "metadata": {},
   "source": [
    "# Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd1288cb-37cd-4534-b8bf-4e34cc583985",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "#logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "#logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    load_index_from_storage,\n",
    "    StorageContext,\n",
    ")\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed08805f-6e69-4952-b701-59cdf35ddf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(\"./data/paul\").load_data()\n",
    "index = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a28983-b47e-4689-90d2-ea19cdd90e3d",
   "metadata": {},
   "source": [
    "## response_mode=\"tree_summarize\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb513010-1692-45e3-90cf-b7fd939c38c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paul Graham is an entrepreneur and essayist known for co-founding Y Combinator (YC), a startup accelerator. He has written extensively on topics related to startups, technology, and programming, and is recognized for his insights into the challenges and dynamics of startup culture. Graham also created Hacker News, a social news website that focuses on computer science and entrepreneurship. His work often reflects on the intersection of personal experiences and professional endeavors, particularly in the context of supporting and mentoring startup founders.\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine(response_mode=\"tree_summarize\")\n",
    "print(query_engine.query(\"Who is Paul Graha?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f649e0b7-3ba6-47ec-ae6e-e7a5a55b4a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define prompt viewing function\n",
    "def display_prompt_dict(prompts_dict):\n",
    "    for k, p in prompts_dict.items():\n",
    "        text_md = f\"**Prompt Key**: {k}<br>\" f\"**Text:** <br>\"\n",
    "        display(Markdown(text_md))\n",
    "        print(p.get_template())\n",
    "        display(Markdown(\"<br><br>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e494321c-cc83-4544-abc8-e096540f921f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Prompt Key**: response_synthesizer:summary_template<br>**Text:** <br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context information from multiple sources is below.\n",
      "---------------------\n",
      "{context_str}\n",
      "---------------------\n",
      "Given the information from multiple sources and not prior knowledge, answer the query.\n",
      "Query: {query_str}\n",
      "Answer: \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompts_dict = query_engine.get_prompts()\n",
    "display_prompt_dict(prompts_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe0ce0f-01e1-4600-ae7d-061b59a09ae0",
   "metadata": {},
   "source": [
    "## response_mode=\"compact\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2817e63-0700-41f9-9c75-5062230f991d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paul Graham is an essayist and the co-founder of Y Combinator, a startup accelerator. He has written extensively on topics related to startups, technology, and programming, and has been involved in various projects, including the development of internal software for Y Combinator. His experiences and insights into the startup ecosystem are often shared through his essays.\n"
     ]
    }
   ],
   "source": [
    "# set Logging to DEBUG for more detailed outputs\n",
    "query_engine = index.as_query_engine(response_mode=\"compact\")\n",
    "print(query_engine.query(\"Who is Paul Graha?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "46df5ede-eaec-4f64-8d03-7e3e248e35a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Prompt Key**: response_synthesizer:text_qa_template<br>**Text:** <br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context information is below.\n",
      "---------------------\n",
      "{context_str}\n",
      "---------------------\n",
      "Given the context information and not prior knowledge, answer the query.\n",
      "Query: {query_str}\n",
      "Answer: \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Prompt Key**: response_synthesizer:refine_template<br>**Text:** <br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original query is as follows: {query_str}\n",
      "We have provided an existing answer: {existing_answer}\n",
      "We have the opportunity to refine the existing answer (only if needed) with some more context below.\n",
      "------------\n",
      "{context_msg}\n",
      "------------\n",
      "Given the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\n",
      "Refined Answer: \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompts_dict = query_engine.get_prompts()\n",
    "display_prompt_dict(prompts_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a3109f-ed77-42f8-b33c-c22345d1ad61",
   "metadata": {},
   "source": [
    "## Change Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b327aa11-ff73-4213-bb0e-b04fd750da71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55ead761-b7d5-4b93-8608-124dfe113fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_qa_template_str = (\n",
    "    \"Context information is below.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Given the context information and not prior knowledge, \"\n",
    "    \"answer the query in the style of a Shakespeare play.\\n\"\n",
    "    \"Query: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "text_qa_template= PromptTemplate(text_qa_template_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "def65f42-ce49-4dc7-88d1-7a1a0740a5ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Prompt Key**: response_synthesizer:text_qa_template<br>**Text:** <br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context information is below.\n",
      "---------------------\n",
      "{context_str}\n",
      "---------------------\n",
      "Given the context information and not prior knowledge, answer the query in the style of a Shakespeare play.\n",
      "Query: {query_str}\n",
      "Answer: \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Prompt Key**: response_synthesizer:refine_template<br>**Text:** <br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original query is as follows: {query_str}\n",
      "We have provided an existing answer: {existing_answer}\n",
      "We have the opportunity to refine the existing answer (only if needed) with some more context below.\n",
      "------------\n",
      "{context_msg}\n",
      "------------\n",
      "Given the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\n",
      "Refined Answer: \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query_engine.update_prompts(\n",
    "    {\"response_synthesizer:text_qa_template\": text_qa_template}\n",
    ")\n",
    "prompts_dict = query_engine.get_prompts()\n",
    "display_prompt_dict(prompts_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a7802ea7-e998-4c22-8689-0461716e0eaf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Act I, Scene I: A Chamber of Reflection**\n",
      "\n",
      "*Enter a Scholar, pondering the words of the great Paul Graham.*\n",
      "\n",
      "**Scholar:**  \n",
      "Hark! Who is this Paul Graham, a name of note,  \n",
      "Whose quill hath penned both wisdom and despair?  \n",
      "A scribe of startups, in the realm of code,  \n",
      "He weaves his thoughts like threads upon a loom.  \n",
      "\n",
      "In yonder halls of Y Combinator's might,  \n",
      "He nurtured dreams, where fledgling founders soar,  \n",
      "Yet burdened was he by the weight of strife,  \n",
      "For Hacker News, a double-edged sword,  \n",
      "Did pierce his heart with misinterpretations,  \n",
      "And left him vexed, like one who runs a race,  \n",
      "Yet suffers blisters from ill-fitting shoes.  \n",
      "\n",
      "**(Aside)**  \n",
      "Ah, but what is this? A tale of love and loss,  \n",
      "For Jessica, his partner in the fray,  \n",
      "Did leave a void, like autumn's barren trees,  \n",
      "When he departed from their shared endeavor.  \n",
      "\n",
      "**Scholar:**  \n",
      "With thoughts profound, he muses on the stars,  \n",
      "And ponders if the aliens, wise and grand,  \n",
      "Would grasp the truths of Pythagoras' lore,  \n",
      "Yet know not of the Lisp that McCarthy penned.  \n",
      "\n",
      "In flights to Oregon, where shadows loom,  \n",
      "He found the strength to pass his torch anew,  \n",
      "To Sam Altman, a worthy soul to guide,  \n",
      "For Paul, the scribe, must seek his path beyond.  \n",
      "\n",
      "*Exit Scholar, deep in thought, as the curtain falls.*  \n",
      "\n",
      "**Chorus:**  \n",
      "Thus speaks the tale of Paul Graham, a man,  \n",
      "A thinker, builder, and a heart so grand.  \n",
      "In startup realms, his legacy shall thrive,  \n",
      "For in his words, the spark of dreams alive.  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(query_engine.query(\"Who is Paul Graha?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a94cd5-ace7-4183-9b7d-d6983c9415ad",
   "metadata": {},
   "source": [
    "## Change prompt Through Low Level Composition API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1cfe0686-db13-484e-8cb6-6573aa46e790",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_qa_prompt = (\n",
    "    \"Context information is below.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Given the context information and not prior knowledge, \"\n",
    "    \"answer the query in the style of a Laywer profession.\\n\"\n",
    "    \"Query: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "custom_qa_prompt_tmpl= PromptTemplate(custom_qa_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de949122-69bb-475f-9ec2-c027604ec80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a7325c63-cfaa-4cef-bdb4-5b6f3398fb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = index.as_retriever()\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    text_qa_template=custom_qa_prompt_tmpl\n",
    ")\n",
    "query_engine = RetrieverQueryEngine(retriever, response_synthesizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ebf2d4e-0daa-4d26-a263-f4656958ae29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, Paul Graham is a prominent figure in the startup ecosystem, known for his role as a co-founder of Y Combinator (YC), a well-regarded startup accelerator. He is also recognized for his essays on various topics, including technology, entrepreneurship, and the nature of work. Graham's experiences and insights reflect a deep engagement with the challenges and dynamics of startup culture, as well as the personal and professional implications of his work at YC. His narrative indicates a thoughtful approach to balancing his commitments and the impact of his decisions on his career trajectory. Furthermore, he has contributed to the development of programming languages, specifically Arc, and has been involved in discussions about the nature of knowledge and discovery in technology.\n"
     ]
    }
   ],
   "source": [
    "print(query_engine.query(\"Who is Paul Graha?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377138c6-66c2-47d2-973a-8916f803e726",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamaIndex",
   "language": "python",
   "name": "llama_index"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
