{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c1472cb-ab1f-48fc-b8d2-e369baee4557",
   "metadata": {},
   "source": [
    "## Setting LLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d45b36d-757f-4acd-8083-995138095f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0b5c8e0-4619-40e1-9141-f3196290ff92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "#print(\"Open AI - \",os.getenv(\"LITELLM_URL\"),os.getenv(\"OPENAI_API_MODEL\"), os.getenv(\"OPENAI_API_EMBEDDING\"))\n",
    "#print(\"OLLAMA  - \",os.getenv(\"OLLAMA_URL\"),os.getenv(\"OLLAMA_MODEL\"))\n",
    "#print(\"Local OLLAMA - \",os.getenv(\"OLLAMA_LOCAL_URL\"),os.getenv(\"OLLAMA_LOCAL_MODEL\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6c5c71d-7466-40e9-a6d9-e62837f621ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b8a433aa-4f89-4e06-8acd-780e730536bd",
   "metadata": {},
   "source": [
    "### RUN LLM AS OLLAMA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4008df15-e79d-4fb2-8269-08182ae74b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#install OPEN AI LLM, skip if already installed\n",
    "!pipenv install llama-index-llms-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88433662-bed6-4bdf-8991-f02f730792b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\napi_base=os.getenv(\"OLLAMA_URL\")\\nmodel=os.getenv(\"OLLAMA_MODEL\")\\nllm = Ollama(model=model, base_url=api_base,request_timeout=180.0)\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "api_base=os.getenv(\"OLLAMA_LOCAL_URL\")\n",
    "model=os.getenv(\"OLLAMA_LOCAL_MODEL\")\n",
    "Settings.llm = Ollama(model=model, base_url=api_base,request_timeout=120.0)\n",
    "\n",
    "# use remote ollam\n",
    "\"\"\"\n",
    "api_base=os.getenv(\"OLLAMA_URL\")\n",
    "model=os.getenv(\"OLLAMA_MODEL\")\n",
    "llm = Ollama(model=model, base_url=api_base,request_timeout=180.0)\n",
    "\"\"\"\n",
    "#test run\n",
    "#response = llm.complete(\"What is the capital of France?\")\n",
    "#print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34bf597-435b-464d-afc4-49f852d60479",
   "metadata": {},
   "source": [
    "### RUN LLM AS OPEN AI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de10f8e1-1bef-44e4-bb0d-671a5a7ac66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#install OPEN AI LLM, skip if already installed\n",
    "!pipenv install llama-index-llms-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cca4af2b-3cbf-42b5-8a0c-eccb3e796188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "api_base=os.getenv(\"LITELLM_URL\")\n",
    "model=os.getenv(\"OPENAI_API_MODEL\")\n",
    "\n",
    "Settings.llm = OpenAI(\n",
    "    model=model,\n",
    "    api_base = api_base,\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "resp = Settings.llm.complete(\"What is the capital of France?\")\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f57c66-5521-4e24-9197-07ce916f7491",
   "metadata": {},
   "source": [
    "## Setting Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f1d1b49-087e-49e3-906b-da53d09fd1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use open AI embedding\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "api_base=os.getenv(\"LITELLM_URL\")\n",
    "embedding_model=os.getenv(\"OPENAI_API_EMBEDDING\")\n",
    "\n",
    "Settings.embed_model = OpenAIEmbedding(\n",
    "    model_name=embedding_model,\n",
    "    api_base = api_base,\n",
    ")\n",
    "\n",
    "# embed_text = Settings.embed_model.get_text_embedding(\"hello\")\n",
    "# print(f\"{len(embed_text)}, {embed_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4439a47c-da96-4915-b071-ab357434b6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use other embedding\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\"\n",
    ")\n",
    "#embed_text = Settings.embed_model.get_text_embedding(\"hello\")\n",
    "#print(f\"{len(embed_text)}, {embed_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed08805f-6e69-4952-b701-59cdf35ddf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "documents = SimpleDirectoryReader(\"./data/paul\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1cd28b56-ba06-4c70-a66c-67908a4ece59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "#logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "#logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82a1d9e-adb4-417b-ac83-f66cc7ef60d0",
   "metadata": {},
   "source": [
    "### KeywordTableIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af7a5212-e416-4459-b2c9-56d2b7b73597",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import KeywordTableIndex\n",
    "\n",
    "index = KeywordTableIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7ae29c5-a222-4b0a-9fe3-a36e3e2f28a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lisp \n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = index.as_query_engine(llm=Settings.llm)\n",
    "print(query.query(\"language\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edca53b-581c-4093-80e0-4a9d33add125",
   "metadata": {},
   "source": [
    "### VectorStoreIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b77d945-4b21-40bc-b758-49153d76931c",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe3fde82-2110-4148-a62e-b2fbe8689e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He wrote a novel about an intelligent computer called Mike.  \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = index.as_query_engine(llm=Settings.llm)\n",
    "print(query.query(\"Who is Paul Graha?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dec4c564-d057-4485-bd22-5938d4614524",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "text_qa_template_str = (\n",
    "    \"Context information is\"\n",
    "    \" below.\\n---------------------\\n{context_str}\\n---------------------\\nUsing\"\n",
    "    \" both the context information and also using your own knowledge, answer\"\n",
    "    \" the question: {query_str}\\nIf the context isn't helpful, you can also\"\n",
    "    \" answer the question on your own.\\n\"\n",
    ")\n",
    "text_qa_template = PromptTemplate(text_qa_template_str)\n",
    "\n",
    "refine_template_str = (\n",
    "    \"The original question is as follows: {query_str}\\nWe have provided an\"\n",
    "    \" existing answer: {existing_answer}\\nWe have the opportunity to refine\"\n",
    "    \" the existing answer (only if needed) with some more context\"\n",
    "    \" below.\\n------------\\n{context_msg}\\n------------\\nUsing both the new\"\n",
    "    \" context and your own knowledge, update or repeat the existing answer.\\n\"\n",
    ")\n",
    "refine_template = PromptTemplate(refine_template_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "603eb256-dd61-430a-aa96-7466166e338d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metadata={'prompt_type': <PromptType.CUSTOM: 'custom'>} template_vars=['context_str', 'query_str'] kwargs={} output_parser=None template_var_mappings=None function_mappings=None template=\"Context information is below.\\n---------------------\\n{context_str}\\n---------------------\\nUsing both the context information and also using your own knowledge, answer the question: {query_str}\\nIf the context isn't helpful, you can also answer the question on your own.\\n\"\n"
     ]
    }
   ],
   "source": [
    "print(text_qa_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11c342d-7ebb-47d9-97dc-e159799826cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    index.as_query_engine(\n",
    "        text_qa_template=text_qa_template,\n",
    "        refine_template=refine_template,\n",
    "        llm=Settings.llm,\n",
    "    ).query(\"Who is Joe Biden?\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51473f09-d57c-4770-81e2-413fc65ecccd",
   "metadata": {},
   "source": [
    "### Tree Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c3f8910f-beab-464d-8787-cd3a968eb708",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import TreeIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f13db65-1f79-4f1a-b1c4-c9ca17d771db",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = TreeIndex.from_documents(documents)\n",
    "query = index.as_query_engine(llm=Settings.llm)\n",
    "print(query.query(\"Who is Paul Graha?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "77c3ce10-3f72-49a0-a351-649477935761",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1d2cff7d-2bd8-4242-b729-1dbf527843a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc ID: e8481d85-7287-4aba-89cc-079e8643ba2c\n",
      "Text: Context LLMs are a phenomenal piece of technology for knowledge\n",
      "generation and reasoning. They are pre-trained on large amounts of\n",
      "publicly available data. How do we best augment LLMs with our own\n",
      "private data? We need a comprehensive toolkit to help perform this\n",
      "data augmentation for LLMs.  Proposed Solution That's where LlamaIndex\n",
      "comes in. Ll...\n"
     ]
    }
   ],
   "source": [
    "print(Document.example())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdaa091-81eb-4198-a201-5409536ba57c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama-index-2",
   "language": "python",
   "name": "llama-index-2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
