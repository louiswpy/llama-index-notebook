{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c1472cb-ab1f-48fc-b8d2-e369baee4557",
   "metadata": {},
   "source": [
    "## Setting LLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0d45b36d-757f-4acd-8083-995138095f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a72f3b23-f340-4408-a4da-e9bee96e76a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "#print(\"Open AI - \",os.getenv(\"LITELLM_URL\"),os.getenv(\"OPENAI_API_MODEL\"), os.getenv(\"OPENAI_API_EMBEDDING\"))\n",
    "#print(\"OLLAMA  - \",os.getenv(\"OLLAMA_URL\"),os.getenv(\"OLLAMA_MODEL\"))\n",
    "#print(\"Local OLLAMA - \",os.getenv(\"OLLAMA_LOCAL_URL\"),os.getenv(\"OLLAMA_LOCAL_MODEL\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6c5c71d-7466-40e9-a6d9-e62837f621ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b8a433aa-4f89-4e06-8acd-780e730536bd",
   "metadata": {},
   "source": [
    "### RUN LLM AS OLLAMA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96176b21-78d2-4446-8e21-3398e1a33730",
   "metadata": {},
   "outputs": [],
   "source": [
    "#install OPEN AI LLM, skip if already installed\n",
    "!pipenv install llama-index-llms-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "88433662-bed6-4bdf-8991-f02f730792b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is **Paris**. ðŸ‡«ðŸ‡·  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "api_base=os.getenv(\"OLLAMA_LOCAL_URL\")\n",
    "model=os.getenv(\"OLLAMA_LOCAL_MODEL\")\n",
    "llm = Ollama(model=model, base_url=api_base,request_timeout=120.0)\n",
    "\n",
    "# use remote ollam\n",
    "\"\"\"\n",
    "api_base=os.getenv(\"OLLAMA_URL\")\n",
    "model=os.getenv(\"OLLAMA_MODEL\")\n",
    "llm = Ollama(model=model, base_url=api_base,request_timeout=180.0)\n",
    "\"\"\"\n",
    "#test run\n",
    "response = llm.complete(\"What is the capital of France?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34bf597-435b-464d-afc4-49f852d60479",
   "metadata": {},
   "source": [
    "### RUN LLM AS OPEN AI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca19e9e-777e-4102-a4d2-95327a8275e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#install OPEN AI LLM, skip if already installed\n",
    "!pipenv install llama-index-llms-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab0492e1-cfed-44e3-9e6e-4852a1920290",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "api_base=os.getenv(\"LITELLM_URL\")\n",
    "model=os.getenv(\"OPENAI_API_MODEL\")\n",
    "\n",
    "Settings.llm = OpenAI(\n",
    "    model=model,\n",
    "    api_base = api_base,\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "#resp = Settings.llm.complete(\"What is the capital of France?\")\n",
    "#print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f57c66-5521-4e24-9197-07ce916f7491",
   "metadata": {},
   "source": [
    "## Setting Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8f1d1b49-087e-49e3-906b-da53d09fd1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use open AI embedding\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "api_base=os.getenv(\"LITELLM_URL\")\n",
    "embedding_model=os.getenv(\"OPENAI_API_EMBEDDING\")\n",
    "\n",
    "Settings.embed_model = OpenAIEmbedding(\n",
    "    model_name=embedding_model,\n",
    "    api_base = api_base,\n",
    ")\n",
    "#embed_text = Settings.embed_model.get_text_embedding(\"hello\")\n",
    "#print(f\"{len(embed_text)}, {embed_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "10a6192c-932a-44be-9462-9ce7c3bf904b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use other embedding\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\"\n",
    ")\n",
    "#embed_text = Settings.embed_model.get_text_embedding(\"hello\")\n",
    "#print(f\"{len(embed_text)}, {embed_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ed08805f-6e69-4952-b701-59cdf35ddf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "documents = SimpleDirectoryReader(\"./data/paul\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1cd28b56-ba06-4c70-a66c-67908a4ece59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "#logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "#logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edca53b-581c-4093-80e0-4a9d33add125",
   "metadata": {},
   "source": [
    "### Node\n",
    "Nodes represent \"chunks\" of source Documents, whether that is a text chunk, an image, or more. They also contain metadata and relationship information with other nodes and index structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "63f61de0-b210-4fc8-aaf9-a8ba55910257",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "parser = SentenceSplitter(chunk_size=1024, chunk_overlap=20)\n",
    "nodes = parser.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "06501aeb-ecc5-4236-8d7c-2228b1ad19be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Nodes: 18\n",
      "Node ID: 35b4b350-e385-4c06-969d-d11ebaeeaece, What I Worked On\n",
      "\n",
      "February 2021\n",
      "\n",
      "Before college th\n",
      "Node ID: 147a7851-770f-4295-a966-53f79b798893, It seemed only a matter of time before we'd have M\n",
      "Node ID: 4339b78b-f145-4566-b9fe-38a8c0b9bead, There were some surplus Xerox Dandelions floating \n",
      "Node ID: 80918e0f-b0bb-4bfb-94f9-395d2b27e172, This was now only weeks away. My nice landlady let\n",
      "Node ID: d45484e8-5de2-4b05-a559-c416aaf3d50f, Our teacher, professor Ulivi, was a nice guy. He c\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total Nodes: {len(nodes)}\")\n",
    "for node in nodes[:5]:\n",
    "    print(f\"Node ID: {node.node_id}, {node.text[0:50]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb75f4d-c4ae-4dfa-bcfc-34ed8973f658",
   "metadata": {},
   "source": [
    "### Node with MetaData Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "efbf0dca-73a9-4212-8e92-1c34be9f94d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.extractors import (\n",
    "    TitleExtractor,\n",
    "    QuestionsAnsweredExtractor,\n",
    ")\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "title_extractor = TitleExtractor(nodes=5)\n",
    "qa_extractor = QuestionsAnsweredExtractor(questions=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c182c39f-d6fb-43e2-ba33-bc47bad361f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume documents are defined -> extract nodes\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[parser, title_extractor, qa_extractor]\n",
    ")\n",
    "nodes = pipeline.run(\n",
    "    documents=documents,\n",
    "    in_place=True,\n",
    "    show_progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "73cc9fe7-8f1c-4d90-ba92-baa554af20f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document_title:\"From Code to Canvas: A Journey of Discovery Through Computing, Art, and Philosophy\"\n",
      "questions_this_excerpt_can_answer:Based on the provided context from the excerpt of Paul Graham's essay, here are three specific questions that can be answered:\n",
      "\n",
      "1. **What programming language did Paul Graham first use when he began programming on the IBM 1401, and what was the method of input for the programs?**\n",
      "   - This question focuses on the specific programming language (Fortran) and the unique method of input (punch cards) that Graham used, which is a detail that may not be widely known or found elsewhere.\n",
      "\n",
      "2. **What was the significance of the TRS-80 in Paul Graham's programming journey, and what types of programs did he create with it?**\n",
      "   - This question highlights the importance of the TRS-80 in Graham's development as a programmer and specifies the kinds of programs he wrote, such as simple games and a word processor, which provides insight into his early programming experiences.\n",
      "\n",
      "3. **What influenced Paul Graham's decision to switch from studying philosophy to artificial intelligence during his college years?**\n",
      "   - This question seeks to uncover the specific influences, such as the novel \"The Moon is a Harsh Mistress\" and a PBS documentary featuring Terry Winograd, that led Graham to change his academic focus, which is a personal anecdote that may not be documented elsewhere.\n"
     ]
    }
   ],
   "source": [
    "for key,value in nodes[0].metadata.items():\n",
    "    if(key in [\"document_title\",\"questions_this_excerpt_can_answer\"]):\n",
    "        print(f\"{key}:{value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880bf67c-8154-41f6-9c53-ad29bf573a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex(nodes)\n",
    "query_engine = index.as_query_engine(response_mode=\"tree_summarize\")\n",
    "print(query_engine.query(\"Who is Paul Graha?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a44974-c95c-452b-8c7e-0f1190c23896",
   "metadata": {},
   "source": [
    "### Semantic Chunker\n",
    "Semantic splitter adaptively picks the breakpoint in-between sentences using embedding similarity. This ensures that a \"chunk\" contains sentences that are semantically related to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d15120-be97-4743-be65-857a8d5c3d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SemanticSplitterNodeParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfc102c-f4e0-4d2d-afd9-2604311f3a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = SemanticSplitterNodeParser(\n",
    "buffer_size=1, breakpoint_percentile_threshold=95, embed_model=Settings.embed_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18ef618-5853-4814-a401-026d36e799e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe8bdf3-470e-49c7-a42d-bd0f0a443f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total Nodes: {len(nodes)}\")\n",
    "for node in nodes[:5]:\n",
    "    print(f\"Node ID: {node.node_id}, {node.get_content()[:100]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5173fab-ab37-4e87-8fc9-3b03d055f186",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex(nodes)\n",
    "query_engine = index.as_query_engine(response_mode=\"tree_summarize\")\n",
    "print(query_engine.query(\"Who is Paul Graha?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecbf5d3-53c9-440f-acbc-fe39a23893f3",
   "metadata": {},
   "source": [
    "### Markdown Node Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3617898-25db-402d-a25b-07d09b3e69a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import MarkdownNodeParser\n",
    "markdown_docs = SimpleDirectoryReader(\"./data/markdown\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dc404c-b39f-448a-8447-6b25c0348ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(markdown_docs[2].text[0:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f624b5e4-1ad8-473d-9d5b-5f6e156589d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = MarkdownNodeParser()\n",
    "nodes = parser.get_nodes_from_documents(markdown_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc6ac0e-a485-4583-8016-dec2d3a983b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total Nodes: {len(nodes)}\")\n",
    "for node in nodes[:5]:\n",
    "    print(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6956f538-f0fb-4b50-8093-70a3c1fd44d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "index = VectorStoreIndex(nodes)\n",
    "query_engine = index.as_query_engine()\n",
    "print(query_engine.query(\"Give me types of KPI?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamaIndex",
   "language": "python",
   "name": "llama_index"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
